{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9229372-8e9c-4320-90df-873e624accbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4be225-5111-4e4c-a16c-665b71138f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e71099-4aed-4505-9cf3-16a7c8c4eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8381785e-6912-4549-b0f6-c67c9e693cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path_or_name = \"./models/upstage/TinySolar-248m-4k\"\n",
    "save_path = './models'\n",
    "model_name = 'upstage/TinySolar-248m-4k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c4cfd1-217e-4417-9645-93c580e342af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\", # change to auto if you have access to a GPU\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir = save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "656bc1ef-24b3-4e60-8ea5-ebd1d13def85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir = save_path,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4a9bd18-0ade-4540-8217-047278c34364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/tokenizer_config.json',\n",
       " './models/special_tokens_map.json',\n",
       " './models/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save them locally\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d981f-367f-48ac-8577-545f539951be",
   "metadata": {},
   "source": [
    "## generate text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84b61ab3-7e2c-4739-86c5-9985d1de058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"I am an engineer. I love\"\n",
    "prompt = \"what is swamy vivekanand kriya yoga\"\n",
    "# prompt = \"इस नदी की धार म ठंडी हवा आती तो ह,ै\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e743ddf9-2609-47c7-acf0-d2eee69a5270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   767,   349,  1719, 27313,   363,   495,  9763,   391,   446,\n",
       "           373,  5157, 21615]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42529c5a-9168-4503-875e-fcedc7998a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['input_ids'][0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "038aef18-1c08-4188-b5fc-20797c92d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 == <s>\n",
      "767 == what\n",
      "349 == is\n",
      "1719 == sw\n",
      "27313 == amy\n",
      "363 == v\n",
      "495 == ive\n",
      "9763 == kan\n",
      "391 == and\n",
      "446 == k\n",
      "373 == ri\n",
      "5157 == ya\n",
      "21615 == yoga\n"
     ]
    }
   ],
   "source": [
    "for key in inputs['input_ids'][0]:\n",
    "   print(f'{key} == {tokenizer.decode([key])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c19ef325-c8fb-4647-880e-58ebd612a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer,\n",
    "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
    "    skip_special_tokens=True,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b839c800-59a1-47c4-8474-0e3ae439bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "The 10 Best Things To Do In The Philippines For A Weekend Getaway\n",
      "By: Katie Mills\n",
      "When you're planning a trip to the Philippines, it can be hard to decide which one is your favorite. But if you're looking for a great place to stay in the country, here are some of the best things to do in the Philippines that will make your vacation memorable and memorable.\n",
      "Best Places To Visit In The Philippines For A Weekend\n",
      "By: Katie Mills\n",
      "If you're looking for a place to visit in the Philippines, then you've come to\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False, \n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97f75de1-0464-4c1b-9f2c-dac14ce562b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs\n",
    "# for key in outputs[0]:\n",
    "#    # print(f'{key} == {tokenizer.decode([key])}')\n",
    "#     print(f'{tokenizer.decode([key])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c996eb-1057-440c-8549-4ed5bb3aa83f",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6e9ebff-2fb7-4dbd-8180-a670232466f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf4llm==0.0.17\n",
      "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pymupdf>=1.24.10 (from pymupdf4llm==0.0.17)\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
      "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf, pymupdf4llm\n",
      "Successfully installed pymupdf-1.25.3 pymupdf4llm-0.0.17\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "! pip install pymupdf4llm==0.0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b9f45b6-b5aa-4ced-9254-7d4512b3794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62ce8eb7-b23c-4624-98f4-1090410df6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'Complete_Works_of_Swami_Vivekananda_all_volumes.pdf'\n",
    "md_text = pymupdf4llm.to_markdown(pdf_path,show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ad14510-f05e-41d3-9994-2911fdb57305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# md_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d541d38-5bb3-4107-ba53-4dbbed812e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "my_dataset = []\n",
    "my_dataset.append(\n",
    "        {'text':md_text }\n",
    "    )\n",
    "my_dataset = datasets.Dataset.from_list(my_dataset)\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c3932716-e73c-4424-8792-c2303f4698e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0e0826c8e7406089f199e701ded4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8279628"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"preprocessed_dataset.parquet\"\n",
    "my_dataset.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7019ab71-43e2-4218-95e8-443747060925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dataset.to_list()\n",
    "# import datasets\n",
    "\n",
    "# dataset = datasets.load_dataset(\n",
    "#     \"parquet\", \n",
    "#     data_files=\"./data/preprocessed_dataset.parquet\", \n",
    "#     split=\"train\"\n",
    "# )\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963431b-4398-4c16-8e6b-ec1cc1ba2ca5",
   "metadata": {},
   "source": [
    "## data packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9090572f-0b97-4081-97c9-addaf52f465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5916b1-964c-45d4-8c48-3c3f1ca54311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3281de2a-3045-4d4e-99cf-960897496d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "my_dataset = datasets.load_dataset(\n",
    "    \"parquet\", \n",
    "    data_files=\"./preprocessed_dataset.parquet\", \n",
    "    split=\"train\"\n",
    ")\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b308bd8a-b966-43e1-9865-1482cc7cb229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e5c747b-05c9-483d-a2f2-f1dc0aa7356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'upstage/SOLAR-10.7B-v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecb57316-8b17-4cd3-b056-313dbdfaef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f99ecf-eed0-4346-9804-db4a7e673b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I', \"'\", 'm', '▁a', '▁short', '▁sentence']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I'm a short sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cc0761e-3dbd-418a-a1e2-6385ff68652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(example[\"text\"])\n",
    "\n",
    "    # Convert tokens to ids\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Add <bos>, <eos> tokens to the front and back of tokens_ids \n",
    "    # bos: begin of sequence, eos: end of sequence\n",
    "    token_ids = [\n",
    "        tokenizer.bos_token_id] \\\n",
    "        + token_ids \\\n",
    "        + [tokenizer.eos_token_id\n",
    "    ]\n",
    "    example[\"input_ids\"] = token_ids\n",
    "\n",
    "    # We will be using this column to count the total number of tokens \n",
    "    # in the final dataset\n",
    "    example[\"num_tokens\"] = len(token_ids)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8de79504-7784-4fa8-b2d1-712d636fdaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d416775c49c4839b7fef61c8ca12040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'num_tokens'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "my_dataset = my_dataset.map(tokenization, load_from_cache_file=False)\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5d4340a-2b96-419b-b825-3775f9bf15c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text -----\n",
      "\n",
      "# Complete Works of Swa\n",
      "\n",
      "input_ids [1, 20041, 13, 13, 28771, 21929, 19012, 302, 3904, 6449, 550, 495, 9763, 5904, 13, 13, 27332, 3904, 6449, 550, 495, 9763, 5904, 13, 13, 8193, 28747, 3550, 1508, 3212]\n",
      "\n",
      "num_tokens 2025559\n"
     ]
    }
   ],
   "source": [
    "sample = my_dataset[0]\n",
    "\n",
    "print(\"text\", sample[\"text\"][:30]) # \n",
    "print(\"\\ninput_ids\", sample[\"input_ids\"][:30])\n",
    "print(\"\\nnum_tokens\", sample[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6aa7176e-4e74-43e4-b388-236ef2ad92b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2025559"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(my_dataset[0][\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59fe9a27-b07c-4ac5-ab47-0bd4fa27474b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'num_tokens'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76f907f6-9dec-443a-9882-8cf3a8cb59ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025559\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.concatenate(my_dataset[\"input_ids\"])\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0b0b099-e3f4-4ef2-8256-0a6830fb51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7faeb501-06e7-4e78-bca6-c4c0fe5f787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025472\n"
     ]
    }
   ],
   "source": [
    "total_length = len(input_ids) - len(input_ids) % max_seq_length\n",
    "print(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "879f41a6-a15c-4cb5-856f-f61c42d346b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2025472,)\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids[:total_length]\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc4e2ce9-30b3-4e5f-8c89-e4d2e5be687e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15824, 128)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_reshaped = input_ids.reshape(-1, max_seq_length).astype(np.int32)\n",
    "input_ids_reshaped.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98834acc-731a-4567-b3ea-57ce81361947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 15824\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "input_ids_list = input_ids_reshaped.tolist()\n",
    "packaged_pretrain_dataset = datasets.Dataset.from_dict(\n",
    "    {\"input_ids\": input_ids_list}\n",
    ")\n",
    "print(packaged_pretrain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca77dc6d-622b-40be-8af7-fcc912ad3e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b75299f416b4e9e8fd6159fe4986e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8165184"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packaged_pretrain_dataset.to_parquet(\"./packaged_pretrain_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb395f0f-df25-4995-899d-900b24d80cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
